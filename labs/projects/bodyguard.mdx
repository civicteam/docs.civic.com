---
title: Bodyguard
description: LLM-based threat detection for prompts and tool calls
public: true
---

**Flask Status: Distilling** ðŸ§ª

## Overview

Bodyguard is a security analysis service that detects malicious prompts and potential attacks in LLM inputs.
It identifies prompt injection attacks, social engineering attempts,
and other security threats before they can reach your AI systems or tools.
Each prompt receives a threat score from 0 to 1, where 1 indicates extreme risk,
and a set of findings, that can aid decision-making.

Unlike rule-based security systems, Bodyguard uses an LLM to understand the semantic intent behind prompts,
catching sophisticated attacks that might bypass traditional filters.

## How It Works

Bodyguard uses advanced language models to understand the semantic intent behind prompts, providing threat analysis that goes beyond simple pattern matching. Each analysis returns:
- A threat score (0-1) indicating risk level
- Detailed findings explaining detected threats
- Recommendations for handling the prompt

## Use Cases

- End-user-facing chatbots, help-bots, etc
- Automatic email readers/responders
- Automatic processors of any untrusted input
- Twitterbots, Discord bots, or any other social media automation

## Key Features

![Bodyguard](/images/labs/projects/bodyguard/bodyguard-summary.png)

- **Prompt Injection Detection**: Identifies attempts to override system prompts or instructions
- **Information Extraction Prevention**: Catches attempts to extract sensitive data or system information
- **Malicious Function Call Detection**: Recognizes dangerous tool-calling patterns
- **Social Engineering Defense**: Detects manipulation tactics and deceptive requests
- **Threat Scoring**: Provides numerical threat scores (0-1) for risk assessment
- **Multiple Deployment Options**: Available as CLI tool, HTTP server, and client library

## Integration Options

Bodyguard is designed for flexible deployment:

### Standalone Service
Run Bodyguard as an independent security service that any application can query for threat analysis.

### SDK Integration
Embed threat detection directly into your applications with native language SDKs.

### Middleware Layer
Deploy as a protective barrier between users and your AI systems, automatically filtering malicious inputs.

### MCP Integration
Seamlessly integrate with MCP servers to protect tool execution from harmful prompts.

## Integration Notes

Bodyguard works as a complementary layer with other Civic Labs tools:

By implementing the **MCP Hooks** interface, Bodyguard can be used to wrap MCP server responses, providing protection against external inputs

Use with **Guardrail Proxy** for defense in depth, Bodyguard analyzes prompts while Guardrail enforces rules

Deploy before **MCP Hub** to pre-screen all requests to your MCP tools

Integrate with **Civic Knowledge** to protect internal systems and LLMs from potentially dangerous data sources

## Status

This flask is currently **distilling**:
Bodyguard is actively deployed in test environments and showing strong detection rates for common attack patterns.
We're expanding the threat detection capabilities and optimizing response times.
Docker images are available for easy deployment. [Contact us](/labs/feedback) if you'd like to test it with your specific use cases.

## Technical Approach

### Threat Detection Engine
Our detection engine analyzes prompts across multiple dimensions:
- **Semantic Analysis**: Understanding intent beyond keywords
- **Context Awareness**: Considering the target system and tools
- **Pattern Recognition**: Identifying known attack signatures
- **Behavioral Analysis**: Detecting unusual request patterns

### Security Philosophy
Bodyguard operates on the principle of defense in depth:
1. **Preventive**: Block threats before they reach your systems
2. **Detective**: Identify and log suspicious activities
3. **Adaptive**: Learn from new attack patterns
4. **Transparent**: Provide clear explanations for decisions

## Getting Started

Interested in protecting your AI systems with Bodyguard?

<Card title="Request Early Access" icon="shield-virus" href="/labs/feedback">
  Get access to Bodyguard and start securing your AI applications
</Card>

## Learn More

- [Prompt Injection Threats](/labs/concepts/prompt-injection) - Understand the risks
- [Security Architecture](/labs/architecture) - Our security model
- [Integration Guide](/labs/feedback) - Request detailed documentation